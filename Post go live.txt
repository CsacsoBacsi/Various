- Cloud SQL - cloud traditional, relational db offering.
  3 flavours. SQL Server, MySQL, Postgres. Postgres #1 in 2017,18, 2020, open source, we chose it
- High availability: primary instance and a standby instance.
- Accommodates udm. Aim: one version of truth. Common structure defined by HSBC modellers. 
  All the sources we ingest transform and make them conform to udm entities
- It is in 3rd normal form: is a database schema design approach for relational databases which uses normalizing
  principles to reduce the duplication of data, avoid data anomalies, ensure referential integrity
- Consistency, integrity, coherency is the key:  
  *** Show pic GTRF *** entities: referential integrity. E.g. claim references transaction event which references
  customer account that eventually references party. Can not happen that we have a claim or ledger entry without
  its parents. Valid ledger account type only!
- Traditional db wich compliant with ACID rules: (Atomicity, Consistency, Isolation, Durability)
     ** either all of the transaction succeeds or none of it does **
     All data will be valid according to all defined rules, including any constraints and triggers that have been
     applied on the database
     all transactions will occur in isolation. They can not reach out to another one until the other succeded
     committed transaction remain in the system, stored permanently
- These rules are followed in our pipeline: Single message populates 5-6 entities if 1 fails - all fails
- Data is immutable. We only do inserts. This way by not overwriting data can do trends and do 'as of date' queries
  Data can never be corrupted, at any point in time shows a consistent picture.
  We use triggers only to translate natural keys into surrogates. 5-6 columns together
  make a row unique. Simple numbers as identifiers make identifying rows much easier.
- Bottleneck. All data comes parallel, multiple workers, multiple pub/sub topics relay messages but we must do
  things in series. Can not add parties to the party entity in parallel! Seems to be a bottle neck.
  We did not experience. Vertically scaleable, more processors. Horizontally: read replicas for SELECTs
  60 000 IOPS input/output per sec. JSON parsing, standardization takes time so we do not expect 1000s of inserts per sec. 

- We also have BigQuery. Analyze petabytes of data using ANSI SQL, this storage is pretty cheap. Sub-second query
  response time and high concurrency. We can leverage Pub/Sub and Dataflow to stream data into BigQuery. Latest data
  becomes available immediately with streaming.
- Lacks referential integrity constraints, no transactions
- Immutability preserved and BQ only supports updates in a limited fashion.
- Excels though in analytic queries, aggregations.


- Make up for lack of integrity, we tied udm to BQ. Only rows (messages) validated by udm can make it to BQ!! 
  At any point in time they are in sync! udm gives a thumbs up, only then do we send the info to BQ. It can be the
  same as the udm entity or denorm tables where multiple tables are joined (which BQ is not so good at) into micro-domain.
  E.g for a party it is not just basic party info but names, address, extra bit of attributes such as date of inc, etc. 
  Full picture!
- You might say the udm could be regarded as just another consumer and untie udm and BQ. No need for uniqueness checks,
  dupes can be handled without performance degradation, which is true. Overall cheap storage costs add up. More processing
  power adds up and danger of having two versions (multiple versions) of truth.
- There is an ongoing review. 

